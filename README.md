# Exploring the Adaptability of Humans and Reinforcement Learning Agents in Altered Game Environments: A Flappy Bird Case Study
## Intrudction
The study of learning and adaptation processes in humans and reinforcement learning (RL) agents is essential for developing intelligent systems capable of adapting to new or changing environments. Flappy Bird, a popular game where players control a bird to navigate through gaps between vertical pipes, serves as an excellent platform to investigate these processes. In this study, we design an experiment involving human participants and an RL agent to explore their adaptability and the role of prior knowledge.\\

We have designed three variations of the Flappy Bird game with altered settings. The first variation features a narrower distance between pipes, requiring minor adjustments to gameplay. The second variation introduces inverted gravity, causing the bird to ascend with gravity and descend with each tap, challenging players' intuition and the RL agent's reward state space. The third variation presents a complex background, incorporating fake pipes that resemble the original pipes but do not result in collisions with the bird. These fake pipes remain stationary, while the real pipes continue to move.

These settings were purposefully designed to test adaptability. We hypothesize that both humans and RL agents can quickly adapt to the narrower pipe distance as it presents minimal changes to the gameplay. Adapting to inverted gravity may take longer for both humans and RL agents due to its counterintuitive nature and necessary adjustments to the agent's reward state space. We expect humans to easily adapt to the complex background, as they can distinguish between fake and real pipes based on their movement, while we anticipate that RL agents may struggle to differentiate between the two due to similarities in the feature maps generated by the convolutional neural networks (CNN). The following experiments will test these assumptions.

Human participants first play the original Flappy Bird game for 10 rounds and then proceed to one of the altered-setting games for 15 rounds. The scores from each round are recorded for analysis. The RL agent, on the other hand, is trained in the default environment until the mean reward approaches 1, and this baseline model is saved. The agent is then exposed to an altered environment, where it fine-tunes the baseline model to adapt to the new environment. 

This experimental design mimics how human participants experience different games, first by playing the default game settings and then using prior experience to adapt to altered game settings. The findings of this study have potential implications for developing more adaptable AI systems and enhancing human-computer interaction. The following sections detail our methods and models, experimental setup, results, and reference instructions. 

## Getting Started
Here I will explain how to run the game which runs automatically using saved model, also I will breif you about basics of Q Learning, Deep Q learning, Dueling architecture and Prioritized Experience Replay.

### Prerequisites
You will need Python 3.X.X with some packages which you can install direclty using requirements.txt.
> pip install -r requirements.txt

### Running The Game
Use the following command to run the game where '--model' indicates the location of saved DQN model.
> python3 play_game.py --model checkpoints/flappy_best_model.dat

## Deep Q Learning
Q Learning is off policy learning method in reinforcement learning which is a developement over on-policy Temporal Difference control algorithm. Q-learning tries to estimate a state-action value function for target policy that deterministically selects the action of highest value.

The problem with Tradition Q learning is that it is not suitable for continuous environment (like Flappy Bird) where an agent can be in infinite number of states. So it is not feasible to store all states in a grid which we use in tradition Q learning. So we use Deep Q learning in these environments.

Deep Q learning is based on Deep Neural Network which takes current state in the form of image or say continuous value and approximates Q-values for each action based on that state.

![Deep Q Learning](https://cdn-images-1.medium.com/max/800/1*w5GuxedZ9ivRYqM_MLUxOQ.png)

[Take a look at this article which explains Deep Q Learning](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)

### Network Architecture (Dueling Architecture)
Here I have used Dueling architecture to calculate Q values. Q-values correspond to how good it is to be at that state and taking an action at that state Q(s,a). 
So we can decompose Q(s,a) as the sum of:
**V(s)** - the value of being at that state
**A(s)** - the advantage of taking that action at that state (how much better is to take this action versus all other possible actions at that state).

```
Q(s,a) = V(s) + A(s,a)
```

![Dueling Architecure](https://cdn-images-1.medium.com/max/1200/1*FkHqwA2eSGixdS-3dvVoMA.png)

### Prioritized Experience Replay
The idea behind PER was that some experiences may be more important than others for our training, but might occur less frequently. Because we sample the batch uniformly (selecting the experiences randomly) these rich experiences that occur rarely have practically no chance to be selected. We want to take in priority experience where there is a big difference between our prediction and the TD target, since it means that we have a lot to learn about it.

```
pt = |dt| + e
where,
	pt = priority of the experience
	dt = magnitude of TD error
	e = constant assures that priority do not become 0
```


[Take a look at this article which explains Double Dueling and PER](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)

## Acknowledements
* The Game has been taken from this [repository]([https://github.com/sourabhv/FlapPyBird](https://github.com/adityajn105/flappy-bird-deep-q-learning)). Thanks to the author.
