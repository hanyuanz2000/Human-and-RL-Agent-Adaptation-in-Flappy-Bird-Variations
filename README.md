# Exploring the Adaptability of Humans and Reinforcement Learning Agents in Altered Game Environments: A Flappy Bird Case Study
## Introdction
The study of learning and adaptation processes in humans and reinforcement learning (RL) agents is essential for developing intelligent systems capable of adapting to new or changing environments. Flappy Bird, a popular game where players control a bird to navigate through gaps between vertical pipes, serves as an excellent platform to investigate these processes. In this study, we design an experiment involving human participants and an RL agent to explore their adaptability and the role of prior knowledge.

We have designed three variations of the Flappy Bird game with altered settings. The first variation features a narrower distance between pipes, requiring minor adjustments to gameplay. The second variation introduces inverted gravity, causing the bird to ascend with gravity and descend with each tap, challenging players' intuition and the RL agent's reward state space. The third variation presents a complex background, incorporating fake pipes that resemble the original pipes but do not result in collisions with the bird. These fake pipes remain stationary, while the real pipes continue to move.

These settings were purposefully designed to test adaptability. We hypothesize that both humans and RL agents can quickly adapt to the narrower pipe distance as it presents minimal changes to the gameplay. Adapting to inverted gravity may take longer for both humans and RL agents due to its counterintuitive nature and necessary adjustments to the agent's reward state space. We expect humans to easily adapt to the complex background, as they can distinguish between fake and real pipes based on their movement, while we anticipate that RL agents may struggle to differentiate between the two due to similarities in the feature maps generated by the convolutional neural networks (CNN). The following experiments will test these assumptions.

Human participants first play the original Flappy Bird game for 10 rounds and then proceed to one of the altered-setting games for 15 rounds. The scores from each round are recorded for analysis. The RL agent, on the other hand, is trained in the default environment until the mean reward approaches 1, and this baseline model is saved. The agent is then exposed to an altered environment, where it fine-tunes the baseline model to adapt to the new environment. 

This experimental design mimics how human participants experience different games, first by playing the default game settings and then using prior experience to adapt to altered game settings. The findings of this study have potential implications for developing more adaptable AI systems and enhancing human-computer interaction. The following sections detail our methods and models, experimental setup, results, and reference instructions. 

### Prerequisites
You will need Python 3.X.X with some packages which you can install direclty using requirements.txt.
> pip install -r requirements.txt

## Acknowledements
* The Code Skeleton has been taken from this [repository](https://github.com/adityajn105/flappy-bird-deep-q-learning). Thanks to the author.
* This project is a collaboration between me, Kaiwen Dai(kd1860@nyu.edu), Sky Lyu(sl6246@nyu.edu), and Yirong Bian(yb970@nyu.edu).
